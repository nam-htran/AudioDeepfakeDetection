{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "addcf52f-0961-4349-9d3d-99f35756f95b",
    "_uuid": "8e5c329e-7453-4823-b458-c2c0fb3a1617",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:55:56.058506Z",
     "iopub.status.busy": "2025-05-22T08:55:56.058104Z",
     "iopub.status.idle": "2025-05-22T08:56:09.506576Z",
     "shell.execute_reply": "2025-05-22T08:56:09.505769Z",
     "shell.execute_reply.started": "2025-05-22T08:55:56.058473Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.19.11-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from wandb) (4.3.8)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-6.31.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from wandb) (2.11.4)\n",
      "Collecting pyyaml (from wandb)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.6-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from wandb) (78.1.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in d:\\windowsinstallations\\miniconda3\\envs\\audiodeepfakedetection\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.19.11-py3-none-win_amd64.whl (20.8 MB)\n",
      "   ---------------------------------------- 0.0/20.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 5.8/20.8 MB 39.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.7/20.8 MB 59.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 20.8/20.8 MB 54.7 MB/s eta 0:00:00\n",
      "Downloading protobuf-6.31.0-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl (341 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading setproctitle-1.3.6-cp312-cp312-win_amd64.whl (12 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, pyyaml, python-dotenv, protobuf, docker-pycreds, click, gitdb, gitpython, wandb\n",
      "\n",
      "   --- ------------------------------------  1/11 [setproctitle]\n",
      "   ------- --------------------------------  2/11 [sentry-sdk]\n",
      "   ------- --------------------------------  2/11 [sentry-sdk]\n",
      "   ------- --------------------------------  2/11 [sentry-sdk]\n",
      "   ------- --------------------------------  2/11 [sentry-sdk]\n",
      "   ------- --------------------------------  2/11 [sentry-sdk]\n",
      "   ------- --------------------------------  2/11 [sentry-sdk]\n",
      "   ------- --------------------------------  2/11 [sentry-sdk]\n",
      "   ------- --------------------------------  2/11 [sentry-sdk]\n",
      "   ------- --------------------------------  2/11 [sentry-sdk]\n",
      "   ---------- -----------------------------  3/11 [pyyaml]\n",
      "   ---------- -----------------------------  3/11 [pyyaml]\n",
      "   -------------- -------------------------  4/11 [python-dotenv]\n",
      "   ------------------ ---------------------  5/11 [protobuf]\n",
      "   ------------------ ---------------------  5/11 [protobuf]\n",
      "   ------------------ ---------------------  5/11 [protobuf]\n",
      "   ------------------ ---------------------  5/11 [protobuf]\n",
      "   ------------------ ---------------------  5/11 [protobuf]\n",
      "   ------------------------- --------------  7/11 [click]\n",
      "   ------------------------- --------------  7/11 [click]\n",
      "   ----------------------------- ----------  8/11 [gitdb]\n",
      "   ----------------------------- ----------  8/11 [gitdb]\n",
      "   -------------------------------- -------  9/11 [gitpython]\n",
      "   -------------------------------- -------  9/11 [gitpython]\n",
      "   -------------------------------- -------  9/11 [gitpython]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ------------------------------------ --- 10/11 [wandb]\n",
      "   ---------------------------------------- 11/11 [wandb]\n",
      "\n",
      "Successfully installed click-8.2.1 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 protobuf-6.31.0 python-dotenv-1.1.0 pyyaml-6.0.2 sentry-sdk-2.29.1 setproctitle-1.3.6 smmap-5.0.2 wandb-0.19.11\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb python-dotenv\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "wandb.login(key=os.getenv(\"WANDB_API\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\NamHoang\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnamthse182380\u001b[0m (\u001b[33mnamthse182380-fpt-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b04d4f70-ebe7-44b5-ad98-ec52e47a74ec",
    "_uuid": "5953f89b-3b9e-47e6-9b57-26bef7615185",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:09.508302Z",
     "iopub.status.busy": "2025-05-22T08:56:09.507897Z",
     "iopub.status.idle": "2025-05-22T08:56:22.113306Z",
     "shell.execute_reply": "2025-05-22T08:56:22.112657Z",
     "shell.execute_reply.started": "2025-05-22T08:56:09.508281Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, accuracy_score\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Device Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Paths and Spectrogram Configuration ---\n",
    "REAL_AUDIO_PATH = \"/kaggle/input/the-lj-speech-dataset/LJSpeech-1.1/wavs\"\n",
    "FAKE_AUDIO_PATH = \"/kaggle/input/wavefake-test/generated_audio/common_voices_prompts_from_conformer_fastspeech2_pwg_ljspeech\"\n",
    "SR = 16000\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 128\n",
    "MAX_FRAMES_SPEC = 313\n",
    "FMIN = 0.0\n",
    "FMAX = None\n",
    "APPLY_AUGMENTATION = True\n",
    "NUM_TIME_MASKS = 1\n",
    "NUM_FREQ_MASKS = 1\n",
    "TIME_MASK_MAX_WIDTH = 40\n",
    "FREQ_MASK_MAX_WIDTH = 15\n",
    "NORM_EPSILON = 1e-6\n",
    "MASK_REPLACEMENT_VALUE = 0.0\n",
    "LIMIT_FILES = None\n",
    "TRAIN_RATIO = 0.70\n",
    "VALIDATION_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 20\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# --- WandB Login with Kaggle Secrets ---\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"WandB login successful using Kaggle Secrets.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to WandB via Kaggle Secrets: {e}. Falling back to environment variable or manual login.\")\n",
    "    wandb.login()\n",
    "\n",
    "# --- WandB Initialization for ViT ---\n",
    "wandb.init(\n",
    "    project=\"ASM01_DAT301m\",\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"vit_patch_size\": 16,\n",
    "        \"vit_embed_dim\": 192,\n",
    "        \"vit_depth\": 6,\n",
    "        \"vit_num_heads\": 6,\n",
    "        \"vit_mlp_ratio\": 4.0,\n",
    "        \"vit_drop_rate\": 0.1,\n",
    "        \"vit_attn_drop_rate\": 0.1,\n",
    "        \"cnn_dropout_rate\": 0.4,  # Added for CNN\n",
    "        \"n_mels\": N_MELS,\n",
    "        \"max_frames_spec\": MAX_FRAMES_SPEC,\n",
    "        \"apply_augmentation\": APPLY_AUGMENTATION,\n",
    "        \"num_time_masks\": NUM_TIME_MASKS,\n",
    "        \"num_freq_masks\": NUM_FREQ_MASKS,\n",
    "        \"time_mask_max_width\": TIME_MASK_MAX_WIDTH,\n",
    "        \"freq_mask_max_width\": FREQ_MASK_MAX_WIDTH,\n",
    "    },\n",
    "    name=f\"ViT_CNN_Audio_Deepfake_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7a450ca-7250-4195-8944-c55ffb2f8cf4",
    "_uuid": "e7fc03ca-b718-409a-9fd4-e64f957acf3b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:22.114107Z",
     "iopub.status.busy": "2025-05-22T08:56:22.113930Z",
     "iopub.status.idle": "2025-05-22T08:56:22.123431Z",
     "shell.execute_reply": "2025-05-22T08:56:22.122713Z",
     "shell.execute_reply.started": "2025-05-22T08:56:22.114092Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 1. Data Loading and Preprocessing Functions ---\n",
    "def get_audio_files_and_labels(real_dir, fake_dir, limit_files=None):\n",
    "    real_files = glob.glob(os.path.join(real_dir, '*.wav'))\n",
    "    fake_files = glob.glob(os.path.join(fake_dir, '*.wav'))\n",
    "    print(f\"Found {len(real_files)} real audio files.\")\n",
    "    print(f\"Found {len(fake_files)} fake audio files.\")\n",
    "    if not real_files and not fake_files:\n",
    "        if limit_files is None: print(\"Warning: Could not find audio files in one or both directories.\")\n",
    "    elif not real_files: print(\"Warning: No real audio files found.\")\n",
    "    elif not fake_files: print(\"Warning: No fake audio files found.\")\n",
    "    if limit_files:\n",
    "        print(f\"Limiting files. Attempting to sample up to {limit_files // 2} from each class.\")\n",
    "        real_files = random.sample(real_files, min(limit_files // 2, len(real_files)))\n",
    "        fake_files = random.sample(fake_files, min(limit_files // 2, len(fake_files)))\n",
    "        print(f\"Selected {len(real_files)} real and {len(fake_files)} fake files after limiting.\")\n",
    "    filepaths = real_files + fake_files\n",
    "    labels = [0] * len(real_files) + [1] * len(fake_files)\n",
    "    if not filepaths:\n",
    "        print(\"No files selected. Check limit_files and paths.\")\n",
    "        return [], []\n",
    "    combined = list(zip(filepaths, labels))\n",
    "    random.shuffle(combined)\n",
    "    filepaths_shuffled, labels_shuffled = zip(*combined) if combined else ([], [])\n",
    "    return list(filepaths_shuffled), list(labels_shuffled)\n",
    "\n",
    "def audio_to_melspectrogram(filepath, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS, max_frames=MAX_FRAMES_SPEC, fmin=FMIN, fmax=FMAX):\n",
    "    try:\n",
    "        y, sr_orig = librosa.load(filepath, sr=None)\n",
    "        if sr_orig != sr: y = librosa.resample(y, orig_sr=sr_orig, target_sr=sr)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmin=fmin, fmax=fmax if fmax is not None else sr/2)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        current_frames = log_mel_spectrogram.shape[1]\n",
    "        if current_frames < max_frames:\n",
    "            pad_value = log_mel_spectrogram.min()\n",
    "            pad_width = max_frames - current_frames\n",
    "            padded_log_mel_spectrogram = np.pad(log_mel_spectrogram, ((0, 0), (0, pad_width)), mode='constant', constant_values=pad_value)\n",
    "            return padded_log_mel_spectrogram\n",
    "        elif current_frames > max_frames:\n",
    "            truncated_log_mel_spectrogram = log_mel_spectrogram[:, :max_frames]\n",
    "            return truncated_log_mel_spectrogram\n",
    "        else: return log_mel_spectrogram\n",
    "    except Exception as e: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de66c623-fe80-460d-8be2-2931cc855bf4",
    "_uuid": "20b897c4-9d37-49c1-8e2b-9f4c881af2c4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:22.125269Z",
     "iopub.status.busy": "2025-05-22T08:56:22.125077Z",
     "iopub.status.idle": "2025-05-22T08:56:22.143629Z",
     "shell.execute_reply": "2025-05-22T08:56:22.143096Z",
     "shell.execute_reply.started": "2025-05-22T08:56:22.125254Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. PyTorch Dataset ---\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, filepaths, labels, transform_spectrogram_fn, augment=False, is_vit_input=False, time_mask_max_width=TIME_MASK_MAX_WIDTH, freq_mask_max_width=FREQ_MASK_MAX_WIDTH, num_time_masks=NUM_TIME_MASKS, num_freq_masks=NUM_FREQ_MASKS, mask_replacement_value=MASK_REPLACEMENT_VALUE):\n",
    "        self.filepaths = filepaths\n",
    "        self.labels = labels\n",
    "        self.transform_spectrogram_fn = transform_spectrogram_fn\n",
    "        self.augment = augment\n",
    "        self.is_vit_input = is_vit_input\n",
    "        self.time_mask_max_width = time_mask_max_width\n",
    "        self.freq_mask_max_width = freq_mask_max_width\n",
    "        self.num_time_masks = num_time_masks\n",
    "        self.num_freq_masks = num_freq_masks\n",
    "        self.mask_replacement_value = mask_replacement_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def _apply_time_mask(self, spectrogram):\n",
    "        augmented_spec = np.copy(spectrogram)\n",
    "        num_frames = augmented_spec.shape[1]\n",
    "        for _ in range(self.num_time_masks):\n",
    "            if self.time_mask_max_width > 0 and num_frames > self.time_mask_max_width:\n",
    "                t = random.randint(1, self.time_mask_max_width)\n",
    "                t0 = random.randint(0, num_frames - t)\n",
    "                augmented_spec[:, t0:t0 + t] = self.mask_replacement_value\n",
    "        return augmented_spec\n",
    "\n",
    "    def _apply_freq_mask(self, spectrogram):\n",
    "        augmented_spec = np.copy(spectrogram)\n",
    "        num_mels = augmented_spec.shape[0]\n",
    "        for _ in range(self.num_freq_masks):\n",
    "            if self.freq_mask_max_width > 0 and num_mels > self.freq_mask_max_width:\n",
    "                f = random.randint(1, self.freq_mask_max_width)\n",
    "                f0 = random.randint(0, num_mels - f)\n",
    "                augmented_spec[f0:f0 + f, :] = self.mask_replacement_value\n",
    "        return augmented_spec\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self.filepaths[idx]\n",
    "        label = self.labels[idx]\n",
    "        mel_spec = self.transform_spectrogram_fn(filepath)\n",
    "        if mel_spec is None: return None\n",
    "        if self.augment:\n",
    "            mel_spec = self._apply_time_mask(mel_spec)\n",
    "            mel_spec = self._apply_freq_mask(mel_spec)\n",
    "        mean = np.mean(mel_spec); std = np.std(mel_spec)\n",
    "        mel_spec_normalized = (mel_spec - mean) / (std + NORM_EPSILON)\n",
    "        if self.is_vit_input:\n",
    "            mel_spec_final = np.stack([mel_spec_normalized]*3, axis=0)\n",
    "        else:\n",
    "            mel_spec_final = np.expand_dims(mel_spec_normalized, axis=0)\n",
    "        mel_spec_tensor = torch.tensor(mel_spec_final, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return mel_spec_tensor, label_tensor\n",
    "\n",
    "def collate_fn_skip_none_vit(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return torch.empty((0, 3, N_MELS, MAX_FRAMES_SPEC)), torch.empty((0,))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "def collate_fn_skip_none_cnn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return torch.empty((0, 1, N_MELS, MAX_FRAMES_SPEC)), torch.empty((0,))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1362df93-fb57-4754-b88f-25348a0a79d4",
    "_uuid": "3b2afb70-d740-4168-9f4d-510051fb06ba",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:22.144537Z",
     "iopub.status.busy": "2025-05-22T08:56:22.144340Z",
     "iopub.status.idle": "2025-05-22T08:56:22.497950Z",
     "shell.execute_reply": "2025-05-22T08:56:22.497226Z",
     "shell.execute_reply.started": "2025-05-22T08:56:22.144517Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 3. Data Splitting ---\n",
    "filepaths_all, labels_all = get_audio_files_and_labels(REAL_AUDIO_PATH, FAKE_AUDIO_PATH, limit_files=LIMIT_FILES)\n",
    "if not filepaths_all: raise ValueError(\"Halting: No audio files found or loaded.\")\n",
    "print(f\"\\nTotal samples before splitting: {len(filepaths_all)} (Labels: Real={labels_all.count(0)}, Fake={labels_all.count(1)})\")\n",
    "X_train_paths, X_temp_paths, y_train, y_temp = train_test_split(filepaths_all, labels_all, test_size=(VALIDATION_RATIO + TEST_RATIO), random_state=SEED, stratify=labels_all if len(set(labels_all)) > 1 else None)\n",
    "if X_temp_paths:\n",
    "    relative_test_ratio = TEST_RATIO / (VALIDATION_RATIO + TEST_RATIO)\n",
    "    if len(set(y_temp)) > 1 : X_val_paths, X_test_paths, y_val, y_test = train_test_split(X_temp_paths, y_temp, test_size=relative_test_ratio, random_state=SEED, stratify=y_temp)\n",
    "    else: X_val_paths, X_test_paths, y_val, y_test = train_test_split(X_temp_paths, y_temp, test_size=relative_test_ratio, random_state=SEED)\n",
    "else: X_val_paths, X_test_paths, y_val, y_test = [], [], [], []\n",
    "print(f\"Training samples: {len(X_train_paths)} (Real={y_train.count(0)}, Fake={y_train.count(1)})\")\n",
    "print(f\"Validation samples: {len(X_val_paths)} (Real={y_val.count(0)}, Fake={y_val.count(1)})\")\n",
    "print(f\"Test samples: {len(X_test_paths)} (Real={y_test.count(0)}, Fake={y_test.count(1)})\")\n",
    "\n",
    "# Log dataset statistics to WandB\n",
    "wandb.log({\n",
    "    \"total_samples\": len(filepaths_all),\n",
    "    \"real_samples\": labels_all.count(0),\n",
    "    \"fake_samples\": labels_all.count(1),\n",
    "    \"train_samples\": len(X_train_paths),\n",
    "    \"train_real\": y_train.count(0),\n",
    "    \"train_fake\": y_train.count(1),\n",
    "    \"val_samples\": len(X_val_paths),\n",
    "    \"val_real\": y_val.count(0),\n",
    "    \"val_fake\": y_val.count(1),\n",
    "    \"test_samples\": len(X_test_paths),\n",
    "    \"test_real\": y_test.count(0),\n",
    "    \"test_fake\": y_test.count(1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d24a1664-45b7-4517-8864-8a071fd5c9b0",
    "_uuid": "ebbee5be-8cd8-4271-b116-c686b5ea8139",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:22.498936Z",
     "iopub.status.busy": "2025-05-22T08:56:22.498759Z",
     "iopub.status.idle": "2025-05-22T08:56:43.891246Z",
     "shell.execute_reply": "2025-05-22T08:56:43.890475Z",
     "shell.execute_reply.started": "2025-05-22T08:56:22.498922Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 4. PyTorch Datasets and DataLoaders for ViT and CNN ---\n",
    "# ViT Datasets\n",
    "train_dataset_vit = AudioDataset(\n",
    "    X_train_paths, y_train, transform_spectrogram_fn=audio_to_melspectrogram,\n",
    "    augment=APPLY_AUGMENTATION, is_vit_input=True\n",
    ")\n",
    "val_dataset_vit = AudioDataset(\n",
    "    X_val_paths, y_val, transform_spectrogram_fn=audio_to_melspectrogram,\n",
    "    augment=False, is_vit_input=True\n",
    ")\n",
    "train_loader_vit = DataLoader(\n",
    "    train_dataset_vit, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn_skip_none_vit\n",
    ")\n",
    "val_loader_vit = DataLoader(\n",
    "    val_dataset_vit, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn_skip_none_vit\n",
    ")\n",
    "if X_test_paths:\n",
    "    test_dataset_vit = AudioDataset(\n",
    "        X_test_paths, y_test, transform_spectrogram_fn=audio_to_melspectrogram,\n",
    "        augment=False, is_vit_input=True\n",
    "    )\n",
    "    test_loader_vit = DataLoader(\n",
    "        test_dataset_vit, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn_skip_none_vit\n",
    "    )\n",
    "else:\n",
    "    test_loader_vit = None\n",
    "    print(\"Test set is empty, test_loader_vit not created.\")\n",
    "\n",
    "# CNN Datasets\n",
    "train_dataset_cnn = AudioDataset(\n",
    "    X_train_paths, y_train, transform_spectrogram_fn=audio_to_melspectrogram,\n",
    "    augment=APPLY_AUGMENTATION, is_vit_input=False\n",
    ")\n",
    "val_dataset_cnn = AudioDataset(\n",
    "    X_val_paths, y_val, transform_spectrogram_fn=audio_to_melspectrogram,\n",
    "    augment=False, is_vit_input=False\n",
    ")\n",
    "train_loader_cnn = DataLoader(\n",
    "    train_dataset_cnn, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn_skip_none_cnn\n",
    ")\n",
    "val_loader_cnn = DataLoader(\n",
    "    val_dataset_cnn, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn_skip_none_cnn\n",
    ")\n",
    "if X_test_paths:\n",
    "    test_dataset_cnn = AudioDataset(\n",
    "        X_test_paths, y_test, transform_spectrogram_fn=audio_to_melspectrogram,\n",
    "        augment=False, is_vit_input=False\n",
    "    )\n",
    "    test_loader_cnn = DataLoader(\n",
    "        test_dataset_cnn, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn_skip_none_cnn\n",
    "    )\n",
    "else:\n",
    "    test_loader_cnn = None\n",
    "    print(\"Test set is empty, test_loader_cnn not created.\")\n",
    "\n",
    "# Test DataLoaders\n",
    "if train_loader_vit and len(train_loader_vit) > 0:\n",
    "    print(\"\\n--- Testing ViT Training DataLoader (PyTorch) ---\")\n",
    "    try:\n",
    "        sample_batch_x_vit_pt, sample_batch_y_vit_pt = next(iter(train_loader_vit))\n",
    "        if sample_batch_x_vit_pt.nelement() > 0:\n",
    "            print(f\"PyTorch ViT Train Batch X shape: {sample_batch_x_vit_pt.shape}\")\n",
    "            print(f\"PyTorch ViT Train Batch Y shape: {sample_batch_y_vit_pt.shape}\")\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            img_to_show = sample_batch_x_vit_pt[0, 0, :, :].cpu().numpy()\n",
    "            librosa.display.specshow(img_to_show, sr=SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Sample Spectrogram (ViT Train Batch, Ch 0, Label: {sample_batch_y_vit_pt[0].item():.0f})')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"sample_spectrogram_vit.png\")\n",
    "            wandb.log({\"sample_spectrogram_vit\": wandb.Image(\"sample_spectrogram_vit.png\")})\n",
    "            plt.close()\n",
    "        else: print(\"Sample batch X from ViT DataLoader is empty after filtering.\")\n",
    "    except StopIteration: print(\"Train loader for ViT (PyTorch) is empty or all samples in first batch failed.\")\n",
    "    except Exception as e: print(f\"Error during ViT DataLoader test: {e}\")\n",
    "else: print(\"Train loader for ViT (PyTorch) is not available or empty.\")\n",
    "\n",
    "if train_loader_cnn and len(train_loader_cnn) > 0:\n",
    "    print(\"\\n--- Testing CNN Training DataLoader (PyTorch) ---\")\n",
    "    try:\n",
    "        sample_batch_x_cnn_pt, sample_batch_y_cnn_pt = next(iter(train_loader_cnn))\n",
    "        if sample_batch_x_cnn_pt.nelement() > 0:\n",
    "            print(f\"PyTorch CNN Train Batch X shape: {sample_batch_x_cnn_pt.shape}\")\n",
    "            print(f\"PyTorch CNN Train Batch Y shape: {sample_batch_y_cnn_pt.shape}\")\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            img_to_show = sample_batch_x_cnn_pt[0, 0, :, :].cpu().numpy()\n",
    "            librosa.display.specshow(img_to_show, sr=SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Sample Spectrogram (CNN Train Batch, Label: {sample_batch_y_cnn_pt[0].item():.0f})')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"sample_spectrogram_cnn.png\")\n",
    "            wandb.log({\"sample_spectrogram_cnn\": wandb.Image(\"sample_spectrogram_cnn.png\")})\n",
    "            plt.close()\n",
    "        else: print(\"Sample batch X from CNN DataLoader is empty after filtering.\")\n",
    "    except StopIteration: print(\"Train loader for CNN (PyTorch) is empty or all samples in first batch failed.\")\n",
    "    except Exception as e: print(f\"Error during CNN DataLoader test: {e}\")\n",
    "else: print(\"Train loader for CNN (PyTorch) is not available or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8dca8446-4bd7-4929-8ec2-9e6817fee729",
    "_uuid": "18e463ee-e665-48f4-9598-ab6fde008c71",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:43.892549Z",
     "iopub.status.busy": "2025-05-22T08:56:43.892279Z",
     "iopub.status.idle": "2025-05-22T08:56:43.911748Z",
     "shell.execute_reply": "2025-05-22T08:56:43.911063Z",
     "shell.execute_reply.started": "2025-05-22T08:56:43.892521Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 5. PyTorch Vision Transformer (ViT) Model ---\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=(N_MELS, MAX_FRAMES_SPEC), patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size, img_size[1] // patch_size)\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=(N_MELS, MAX_FRAMES_SPEC), patch_size=16, in_chans=3, num_classes=1,\n",
    "                 embed_dim=192, depth=6, num_heads=6, mlp_ratio=4., qkv_bias=True,\n",
    "                 drop_rate=0.1, attn_drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate)\n",
    "            for i in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "46187e48-f7b7-4638-9567-763fc52d16fb",
    "_uuid": "9ab249ee-6255-47d0-9198-893c539edc2c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:43.912844Z",
     "iopub.status.busy": "2025-05-22T08:56:43.912577Z",
     "iopub.status.idle": "2025-05-22T08:56:43.930465Z",
     "shell.execute_reply": "2025-05-22T08:56:43.929969Z",
     "shell.execute_reply.started": "2025-05-22T08:56:43.912820Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 6. PyTorch CNN Model ---\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1, dropout_rate=0.4):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop1 = nn.Dropout2d(dropout_rate/2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout2d(dropout_rate/2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop3 = nn.Dropout2d(dropout_rate)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop4 = nn.Dropout2d(dropout_rate)\n",
    "        height_after_convs = N_MELS // (2**4)\n",
    "        width_after_convs = MAX_FRAMES_SPEC // (2**4)\n",
    "        self.fc1 = nn.Linear(256 * height_after_convs * width_after_convs, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.drop_fc1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
    "        self.drop_fc2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.drop3(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "        x = self.drop4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.drop_fc1(x)\n",
    "        x = F.relu(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.drop_fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6661ff23-3d85-46ca-a321-31262a28dda2",
    "_uuid": "db30515d-7084-4747-ba5c-c106bd38f913",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:43.931519Z",
     "iopub.status.busy": "2025-05-22T08:56:43.931234Z",
     "iopub.status.idle": "2025-05-22T08:56:46.590602Z",
     "shell.execute_reply": "2025-05-22T08:56:46.589924Z",
     "shell.execute_reply.started": "2025-05-22T08:56:43.931495Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 7. Initialize Models, Optimizers, Loss ---\n",
    "VIT_PATCH_SIZE = 16\n",
    "VIT_EMBED_DIM = 192\n",
    "VIT_DEPTH = 6\n",
    "VIT_NUM_HEADS = 6\n",
    "VIT_MLP_RATIO = 4.0\n",
    "VIT_DROP_RATE = 0.1\n",
    "VIT_ATTN_DROP_RATE = 0.1\n",
    "CNN_DROPOUT_RATE = 0.4\n",
    "\n",
    "if N_MELS % VIT_PATCH_SIZE != 0 or MAX_FRAMES_SPEC % VIT_PATCH_SIZE != 0:\n",
    "    print(f\"Warning: N_MELS ({N_MELS}) or MAX_FRAMES_SPEC ({MAX_FRAMES_SPEC}) is not perfectly divisible by VIT_PATCH_SIZE ({VIT_PATCH_SIZE}).\")\n",
    "    print(\"The PatchEmbed layer will crop the input to the largest divisible dimensions.\")\n",
    "    eff_H = (N_MELS // VIT_PATCH_SIZE) * VIT_PATCH_SIZE\n",
    "    eff_W = (MAX_FRAMES_SPEC // VIT_PATCH_SIZE) * VIT_PATCH_SIZE\n",
    "    print(f\"Effective input to PatchEmbed will be ({eff_H}, {eff_W})\")\n",
    "\n",
    "pytorch_vit_model = VisionTransformer(\n",
    "    img_size=(N_MELS, MAX_FRAMES_SPEC),\n",
    "    patch_size=VIT_PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    num_classes=1,\n",
    "    embed_dim=VIT_EMBED_DIM,\n",
    "    depth=VIT_DEPTH,\n",
    "    num_heads=VIT_NUM_HEADS,\n",
    "    mlp_ratio=VIT_MLP_RATIO,\n",
    "    qkv_bias=True,\n",
    "    drop_rate=VIT_DROP_RATE,\n",
    "    attn_drop_rate=VIT_ATTN_DROP_RATE\n",
    ").to(DEVICE)\n",
    "\n",
    "pytorch_cnn_model = AudioCNN(num_classes=1, dropout_rate=CNN_DROPOUT_RATE).to(DEVICE)\n",
    "\n",
    "optimizer_vit = optim.AdamW(pytorch_vit_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "optimizer_cnn = optim.AdamW(pytorch_cnn_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion_vit = nn.BCEWithLogitsLoss()\n",
    "criterion_cnn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"\\n--- PyTorch ViT Model Architecture (Simplified) ---\")\n",
    "total_params_vit = sum(p.numel() for p in pytorch_vit_model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters (ViT): {total_params_vit:,}\")\n",
    "print(f\"ViT Config: Patch={VIT_PATCH_SIZE}, EmbedDim={VIT_EMBED_DIM}, Depth={VIT_DEPTH}, Heads={VIT_NUM_HEADS}\")\n",
    "\n",
    "print(\"\\n--- PyTorch CNN Model Architecture (Simplified) ---\")\n",
    "total_params_cnn = sum(p.numel() for p in pytorch_cnn_model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters (CNN): {total_params_cnn:,}\")\n",
    "\n",
    "wandb.log({\"vit_total_trainable_parameters\": total_params_vit, \"cnn_total_trainable_parameters\": total_params_cnn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c585841e-8f2b-4313-b854-5d226e2b987b",
    "_uuid": "59106adb-f812-45a6-acef-3b5dc06eafcc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:46.593481Z",
     "iopub.status.busy": "2025-05-22T08:56:46.592410Z",
     "iopub.status.idle": "2025-05-22T08:56:46.607714Z",
     "shell.execute_reply": "2025-05-22T08:56:46.607001Z",
     "shell.execute_reply.started": "2025-05-22T08:56:46.593451Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 8. Training Loop and Evaluation Function ---\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch_num, num_epochs, model_name=\"Model\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch_num+1}/{num_epochs} [{model_name} Training]\", unit=\"batch\")\n",
    "    for inputs, labels in progress_bar:\n",
    "        if inputs.nelement() == 0: continue\n",
    "        inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item(), acc=correct_predictions/total_samples if total_samples > 0 else 0)\n",
    "    epoch_loss = running_loss / total_samples if total_samples > 0 else 0\n",
    "    epoch_acc = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model_pytorch(model, val_loader, criterion, device, epoch_num=None, num_epochs=None, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_labels = []\n",
    "    all_preds_probs = []\n",
    "    desc_str = f\"{model_name} Evaluating\"\n",
    "    if epoch_num is not None and num_epochs is not None: desc_str = f\"Epoch {epoch_num+1}/{num_epochs} [{model_name} Validation]\"\n",
    "    progress_bar = tqdm(val_loader, desc=desc_str, unit=\"batch\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            if inputs.nelement() == 0: continue\n",
    "            inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if loss is not None: running_loss += loss.item() * inputs.size(0)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = probs > 0.5\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "            all_preds_probs.extend(probs.cpu().numpy().flatten())\n",
    "            if loss is not None: progress_bar.set_postfix(loss=loss.item(), acc=correct_predictions/total_samples if total_samples > 0 else 0)\n",
    "            else: progress_bar.set_postfix(acc=correct_predictions/total_samples if total_samples > 0 else 0)\n",
    "    epoch_loss = running_loss / total_samples if total_samples > 0 else float('inf')\n",
    "    epoch_acc = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return epoch_loss, epoch_acc, np.array(all_labels), np.array(all_preds_probs)\n",
    "\n",
    "def plot_history(train_losses, val_losses, train_accs, val_accs, model_name=\"Model\"):\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accs, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accs, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"training_history_{model_name.lower()}.png\")\n",
    "    wandb.log({f\"training_history_{model_name.lower()}\": wandb.Image(f\"training_history_{model_name.lower()}.png\")})\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a3ffabf1-d4cd-4bbd-8086-7239b73e1a8d",
    "_uuid": "5bc9f70b-18ae-458e-a2f3-7d858d4d359c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-22T08:56:46.608847Z",
     "iopub.status.busy": "2025-05-22T08:56:46.608603Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 9. Main Training Execution for ViT ---\n",
    "train_losses_vit_history = []\n",
    "val_losses_vit_history = []\n",
    "train_accs_vit_history = []\n",
    "val_accs_vit_history = []\n",
    "best_val_loss_vit = float('inf')\n",
    "best_epoch_vit = -1\n",
    "patience_counter_vit = 0\n",
    "patience_limit_vit = 7\n",
    "\n",
    "print(f\"\\n--- Starting ViT Model Training on {DEVICE} for {EPOCHS} epochs ---\")\n",
    "start_time_total_vit = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(pytorch_vit_model, train_loader_vit, criterion_vit, optimizer_vit, DEVICE, epoch, EPOCHS, model_name=\"ViT\")\n",
    "    val_loss, val_acc, _, _ = evaluate_model_pytorch(pytorch_vit_model, val_loader_vit, criterion_vit, DEVICE, epoch, EPOCHS, model_name=\"ViT\")\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - ViT - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} - \"\n",
    "          f\"Duration: {epoch_duration:.2f}s\")\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"vit_train_loss\": train_loss,\n",
    "        \"vit_train_accuracy\": train_acc,\n",
    "        \"vit_val_loss\": val_loss,\n",
    "        \"vit_val_accuracy\": val_acc,\n",
    "        \"vit_epoch_duration_seconds\": epoch_duration\n",
    "    })\n",
    "    train_losses_vit_history.append(train_loss)\n",
    "    val_losses_vit_history.append(val_loss)\n",
    "    train_accs_vit_history.append(train_acc)\n",
    "    val_accs_vit_history.append(val_acc)\n",
    "    if val_loss < best_val_loss_vit:\n",
    "        best_val_loss_vit = val_loss\n",
    "        best_epoch_vit = epoch + 1\n",
    "        torch.save(pytorch_vit_model.state_dict(), 'best_vit_model_pytorch.pth')\n",
    "        print(f\"Epoch {epoch+1}: ViT Val loss improved to {val_loss:.4f}. Model saved.\")\n",
    "        wandb.save('best_vit_model_pytorch.pth')\n",
    "        patience_counter_vit = 0\n",
    "    else:\n",
    "        patience_counter_vit += 1\n",
    "        print(f\"Epoch {epoch+1}: ViT Val loss ({val_loss:.4f}) did not improve from {best_val_loss_vit:.4f}. Patience: {patience_counter_vit}/{patience_limit_vit}\")\n",
    "    if patience_counter_vit >= patience_limit_vit:\n",
    "        print(f\"ViT Early stopping triggered at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "total_training_time_vit = time.time() - start_time_total_vit\n",
    "print(f\"--- ViT Training Finished ---\")\n",
    "print(f\"Total ViT Training Time: {total_training_time_vit // 60:.0f}m {total_training_time_vit % 60:.0f}s\")\n",
    "print(f\"Best ViT validation loss: {best_val_loss_vit:.4f} at epoch {best_epoch_vit}\")\n",
    "wandb.log({\n",
    "    \"vit_total_training_time_minutes\": total_training_time_vit / 60,\n",
    "    \"vit_best_val_loss\": best_val_loss_vit,\n",
    "    \"vit_best_epoch\": best_epoch_vit\n",
    "})\n",
    "plot_history(train_losses_vit_history, val_losses_vit_history, train_accs_vit_history, val_accs_vit_history, \"ViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f98e7449-df01-4a67-a40d-bc75d958928e",
    "_uuid": "9f457e45-1b71-4fce-8dbe-6dcf1cb7dd78",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 10. Main Training Execution for CNN ---\n",
    "train_losses_cnn_history = []\n",
    "val_losses_cnn_history = []\n",
    "train_accs_cnn_history = []\n",
    "val_accs_cnn_history = []\n",
    "best_val_loss_cnn = float('inf')\n",
    "best_epoch_cnn = -1\n",
    "patience_counter_cnn = 0\n",
    "patience_limit_cnn = 7\n",
    "\n",
    "print(f\"\\n--- Starting CNN Model Training on {DEVICE} for {EPOCHS} epochs ---\")\n",
    "start_time_total_cnn = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(pytorch_cnn_model, train_loader_cnn, criterion_cnn, optimizer_cnn, DEVICE, epoch, EPOCHS, model_name=\"CNN\")\n",
    "    val_loss, val_acc, _, _ = evaluate_model_pytorch(pytorch_cnn_model, val_loader_cnn, criterion_cnn, DEVICE, epoch, EPOCHS, model_name=\"CNN\")\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - CNN - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} - \"\n",
    "          f\"Duration: {epoch_duration:.2f}s\")\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"cnn_train_loss\": train_loss,\n",
    "        \"cnn_train_accuracy\": train_acc,\n",
    "        \"cnn_val_loss\": val_loss,\n",
    "        \"cnn_val_accuracy\": val_acc,\n",
    "        \"cnn_epoch_duration_seconds\": epoch_duration\n",
    "    })\n",
    "    train_losses_cnn_history.append(train_loss)\n",
    "    val_losses_cnn_history.append(val_loss)\n",
    "    train_accs_cnn_history.append(train_acc)\n",
    "    val_accs_cnn_history.append(val_acc)\n",
    "    if val_loss < best_val_loss_cnn:\n",
    "        best_val_loss_cnn = val_loss\n",
    "        best_epoch_cnn = epoch + 1\n",
    "        torch.save(pytorch_cnn_model.state_dict(), 'best_cnn_model_pytorch.pth')\n",
    "        print(f\"Epoch {epoch+1}: CNN Val loss improved to {val_loss:.4f}. Model saved.\")\n",
    "        wandb.save('best_cnn_model_pytorch.pth')\n",
    "        patience_counter_cnn = 0\n",
    "    else:\n",
    "        patience_counter_cnn += 1\n",
    "        print(f\"Epoch {epoch+1}: CNN Val loss ({val_loss:.4f}) did not improve from {best_val_loss_cnn:.4f}. Patience: {patience_counter_cnn}/{patience_limit_cnn}\")\n",
    "    if patience_counter_cnn >= patience_limit_cnn:\n",
    "        print(f\"CNN Early stopping triggered at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "total_training_time_cnn = time.time() - start_time_total_cnn\n",
    "print(f\"--- CNN Training Finished ---\")\n",
    "print(f\"Total CNN Training Time: {total_training_time_cnn // 60:.0f}m {total_training_time_cnn % 60:.0f}s\")\n",
    "print(f\"Best CNN validation loss: {best_val_loss_cnn:.4f} at epoch {best_epoch_cnn}\")\n",
    "wandb.log({\n",
    "    \"cnn_total_training_time_minutes\": total_training_time_cnn / 60,\n",
    "    \"cnn_best_val_loss\": best_val_loss_cnn,\n",
    "    \"cnn_best_epoch\": best_epoch_cnn\n",
    "})\n",
    "plot_history(train_losses_cnn_history, val_losses_cnn_history, train_accs_cnn_history, val_accs_cnn_history, \"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7d0616a-8aab-4033-be14-0a1e9ac883b4",
    "_uuid": "a49eeb14-1bbf-4872-8701-bed4529f1612",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 11. Evaluation on Test Set for ViT ---\n",
    "if test_loader_vit:\n",
    "    print(\"\\n--- Evaluating ViT on Test Set with the Best Model ---\")\n",
    "    best_model_vit = VisionTransformer(\n",
    "        img_size=(N_MELS, MAX_FRAMES_SPEC), patch_size=VIT_PATCH_SIZE, in_chans=3, num_classes=1,\n",
    "        embed_dim=VIT_EMBED_DIM, depth=VIT_DEPTH, num_heads=VIT_NUM_HEADS, mlp_ratio=VIT_MLP_RATIO,\n",
    "        qkv_bias=True, drop_rate=VIT_DROP_RATE, attn_drop_rate=VIT_ATTN_DROP_RATE\n",
    "    ).to(DEVICE)\n",
    "    try:\n",
    "        best_model_vit.load_state_dict(torch.load('best_vit_model_pytorch.pth', map_location=DEVICE))\n",
    "        print(\"Best ViT model weights loaded successfully.\")\n",
    "        wandb.init(project=\"audio-deepfake-detection\", name=f\"ViT_Test_Evaluation_{time.strftime('%Y%m%d_%H%M%S')}\")\n",
    "        test_loss, test_acc, test_labels_true, test_preds_probs = evaluate_model_pytorch(best_model_vit, test_loader_vit, criterion_vit, DEVICE, model_name=\"ViT\")\n",
    "        print(f\"ViT Test Set - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")\n",
    "        wandb.log({\n",
    "            \"vit_test_loss\": test_loss,\n",
    "            \"vit_test_accuracy\": test_acc\n",
    "        })\n",
    "        if len(test_labels_true) > 0 and len(test_preds_probs) > 0:\n",
    "            test_preds_binary = (test_preds_probs > 0.5).astype(int)\n",
    "            print(\"\\nClassification Report (ViT Test Set):\")\n",
    "            report = classification_report(test_labels_true, test_preds_binary, target_names=['Real (0)', 'Fake (1)'], output_dict=True)\n",
    "            print(classification_report(test_labels_true, test_preds_binary, target_names=['Real (0)', 'Fake (1)']))\n",
    "            wandb.log({\n",
    "                \"vit_test_precision_real\": report['Real (0)']['precision'],\n",
    "                \"vit_test_recall_real\": report['Real (0)']['recall'],\n",
    "                \"vit_test_f1_real\": report['Real (0)']['f1-score'],\n",
    "                \"vit_test_precision_fake\": report['Fake (1)']['precision'],\n",
    "                \"vit_test_recall_fake\": report['Fake (1)']['recall'],\n",
    "                \"vit_test_f1_fake\": report['Fake (1)']['f1-score'],\n",
    "                \"vit_test_macro_avg_precision\": report['macro avg']['precision'],\n",
    "                \"vit_test_macro_avg_recall\": report['macro avg']['recall'],\n",
    "                \"vit_test_macro_avg_f1\": report['macro avg']['f1-score'],\n",
    "                \"vit_test_weighted_avg_precision\": report['weighted avg']['precision'],\n",
    "                \"vit_test_weighted_avg_recall\": report['weighted avg']['recall'],\n",
    "                \"vit_test_weighted_avg_f1\": report['weighted avg']['f1-score']\n",
    "            })\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(test_labels_true, test_preds_probs)\n",
    "                print(f\"ROC AUC Score (ViT Test Set): {roc_auc:.4f}\")\n",
    "                wandb.log({\"vit_test_roc_auc\": roc_auc})\n",
    "            except ValueError as e:\n",
    "                print(f\"Could not calculate ROC AUC for ViT: {e}\")\n",
    "            print(\"\\nConfusion Matrix (ViT Test Set):\")\n",
    "            cm = confusion_matrix(test_labels_true, test_preds_binary)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
    "            disp.plot(cmap=plt.cm.Blues)\n",
    "            plt.title('Confusion Matrix - PyTorch ViT (Test Set)')\n",
    "            plt.savefig(\"confusion_matrix_vit.png\")\n",
    "            plt.close()\n",
    "            wandb.log({\"vit_confusion_matrix\": wandb.Image(\"confusion_matrix_vit.png\")})\n",
    "        else:\n",
    "            print(\"Not enough data in ViT test results for report/matrix.\")\n",
    "        wandb.finish()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'best_vit_model_pytorch.pth' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during ViT test set evaluation: {e}\")\n",
    "else:\n",
    "    print(\"\\nViT Test loader is not available. Skipping test set evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e2c6eb99-ccc5-4e91-aee0-afa3b57e2011",
    "_uuid": "093092cf-9c6b-4d2a-a7a3-50695b75c9ee",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 12. Evaluation on Test Set for CNN ---\n",
    "if test_loader_cnn:\n",
    "    print(\"\\n--- Evaluating CNN on Test Set with the Best Model ---\")\n",
    "    best_model_cnn = AudioCNN(num_classes=1, dropout_rate=CNN_DROPOUT_RATE).to(DEVICE)\n",
    "    try:\n",
    "        best_model_cnn.load_state_dict(torch.load('best_cnn_model_pytorch.pth', map_location=DEVICE))\n",
    "        print(\"Best CNN model weights loaded successfully.\")\n",
    "        wandb.init(project=\"audio-deepfake-detection\", name=f\"CNN_Test_Evaluation_{time.strftime('%Y%m%d_%H%M%S')}\")\n",
    "        test_loss, test_acc, test_labels_true, test_preds_probs = evaluate_model_pytorch(best_model_cnn, test_loader_cnn, criterion_cnn, DEVICE, model_name=\"CNN\")\n",
    "        print(f\"CNN Test Set - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")\n",
    "        wandb.log({\n",
    "            \"cnn_test_loss\": test_loss,\n",
    "            \"cnn_test_accuracy\": test_acc\n",
    "        })\n",
    "        if len(test_labels_true) > 0 and len(test_preds_probs) > 0:\n",
    "            test_preds_binary = (test_preds_probs > 0.5).astype(int)\n",
    "            print(\"\\nClassification Report (CNN Test Set):\")\n",
    "            report = classification_report(test_labels_true, test_preds_binary, target_names=['Real (0)', 'Fake (1)'], output_dict=True)\n",
    "            print(classification_report(test_labels_true, test_preds_binary, target_names=['Real (0)', 'Fake (1)']))\n",
    "            wandb.log({\n",
    "                \"cnn_test_precision_real\": report['Real (0)']['precision'],\n",
    "                \"cnn_test_recall_real\": report['Real (0)']['recall'],\n",
    "                \"cnn_test_f1_real\": report['Real (0)']['f1-score'],\n",
    "                \"cnn_test_precision_fake\": report['Fake (1)']['precision'],\n",
    "                \"cnn_test_recall_fake\": report['Fake (1)']['recall'],\n",
    "                \"cnn_test_f1_fake\": report['Fake (1)']['f1-score'],\n",
    "                \"cnn_test_macro_avg_precision\": report['macro avg']['precision'],\n",
    "                \"cnn_test_macro_avg_recall\": report['macro avg']['recall'],\n",
    "                \"cnn_test_macro_avg_f1\": report['macro avg']['f1-score'],\n",
    "                \"cnn_test_weighted_avg_precision\": report['weighted avg']['precision'],\n",
    "                \"cnn_test_weighted_avg_recall\": report['weighted avg']['recall'],\n",
    "                \"cnn_test_weighted_avg_f1\": report['weighted avg']['f1-score']\n",
    "            })\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(test_labels_true, test_preds_probs)\n",
    "                print(f\"ROC AUC Score (CNN Test Set): {roc_auc:.4f}\")\n",
    "                wandb.log({\"cnn_test_roc_auc\": roc_auc})\n",
    "            except ValueError as e:\n",
    "                print(f\"Could not calculate ROC AUC for CNN: {e}\")\n",
    "            print(\"\\nConfusion Matrix (CNN Test Set):\")\n",
    "            cm = confusion_matrix(test_labels_true, test_preds_binary)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
    "            disp.plot(cmap=plt.cm.Blues)\n",
    "            plt.title('Confusion Matrix - PyTorch CNN (Test Set)')\n",
    "            plt.savefig(\"confusion_matrix_cnn.png\")\n",
    "            plt.close()\n",
    "            wandb.log({\"cnn_confusion_matrix\": wandb.Image(\"confusion_matrix_cnn.png\")})\n",
    "        else:\n",
    "            print(\"Not enough data in CNN test results for report/matrix.\")\n",
    "        wandb.finish()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'best_cnn_model_pytorch.pth' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during CNN test set evaluation: {e}\")\n",
    "else:\n",
    "    print(\"\\nCNN Test loader is not available. Skipping test set evaluation.\")\n",
    "\n",
    "# Close main WandB run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1159053,
     "sourceId": 1942970,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3084682,
     "sourceId": 5306083,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "AudioDeepFakeDetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
